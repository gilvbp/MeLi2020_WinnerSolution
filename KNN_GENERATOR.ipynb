{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from scipy import sparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data pipeline\n",
    "from sklearn.model_selection import StratifiedKFold, KFold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# metrics\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, balanced_accuracy_score\n",
    "from sklearn.metrics import roc_curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "from sklearn.decomposition import PCA, TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# models\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data and other useful stuffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.read_json(\"data/train_dataset.jl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\WindowsPrograms\\envs\\meli\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 23.5 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for sess in data_train[\"user_history\"].values:\n",
    "    for event in sess:\n",
    "        event[\"event_timestamp\"] = np.datetime64(event[\"event_timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_items = pd.read_json(\"data/item_data.jl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.read_json(\"data/test_dataset.jl\", lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\WindowsPrograms\\envs\\meli\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: parsing timezone aware datetimes is deprecated; this will raise an error in the future\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 11.2 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "for sess in data_test[\"user_history\"].values:\n",
    "    for event in sess:\n",
    "        event[\"event_timestamp\"] = np.datetime64(event[\"event_timestamp\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_items.set_index(\"item_id\", inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7893"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_items[\"domain_id\"].nunique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.00016125372631675084"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_items[\"price\"].isna().mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_items_price = data_items[\"price\"]\n",
    "data_items_price.dropna(inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemPrice = data_items_price.to_dict()        \n",
    "\n",
    "itemDomain = data_items[\"domain_id\"].to_dict()\n",
    "\n",
    "for i, d in itemDomain.items():\n",
    "    if d is None:\n",
    "        itemDomain[i] = \"<UNKN>\"\n",
    "\n",
    "itemCondition = data_items[\"condition\"].map({\"new\" : 1, \"used\" : 0, None : -1}).to_dict()\n",
    "\n",
    "for i, d in itemCondition.items():\n",
    "    if d is None:\n",
    "        print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "uhs_train = data_train[\"user_history\"].values\n",
    "target = data_train[\"item_bought\"].values\n",
    "uhs_test= data_test[\"user_history\"].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "domain = np.vectorize(itemDomain.get)(target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "domainCode = dict()\n",
    "for i, dom in enumerate(set(domain)):\n",
    "    domainCode[dom] = i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MLB-CELLPHONES            0.060678\n",
       "MLB-SNEAKERS              0.035357\n",
       "MLB-SUPPLEMENTS           0.023143\n",
       "MLB-HEADPHONES            0.021911\n",
       "MLB-SMARTWATCHES          0.019273\n",
       "                            ...   \n",
       "MLM-MEN_SPORT_SWIMWEAR    0.000002\n",
       "MLM-LIP_GLOSSES           0.000002\n",
       "MLM-FOOTBALL_SOCKS        0.000002\n",
       "MLM-NAIL_SAMPLES          0.000002\n",
       "MLM-SEAT_BELTS            0.000002\n",
       "Length: 3214, dtype: float64"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(domain).value_counts(normalize=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "most_bought_domain = pd.Series(domain).value_counts().idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MLB-CELLPHONES'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "most_bought_domain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "topDomainTop20itens = list(pd.Series(target[domain == most_bought_domain]).value_counts(ascending=False).index[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "domItemFreq = dict()\n",
    "for i, d in zip(target, domain):\n",
    "    domItemFreq[d] = domItemFreq.get(d, list()) + [i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "top20domItems = dict()\n",
    "for d, ilist in domItemFreq.items():\n",
    "    top20domItems[d] = list(pd.Series(ilist).value_counts().index[:20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "itemAsLabelCount = pd.Series(target).value_counts().to_dict()\n",
    "most_sold = pd.Series(itemAsLabelCount).idxmax()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'MLB-HEADPHONES'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "itemDomain[most_sold]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "topDomainTop10itens = list(pd.Series(target[domain == most_bought_domain]).value_counts(ascending=False).index[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[859574,\n",
       " 1371799,\n",
       " 119703,\n",
       " 1332849,\n",
       " 882697,\n",
       " 1098739,\n",
       " 98853,\n",
       " 790888,\n",
       " 967194,\n",
       " 1595373]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topDomainTop10itens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "cumsum = pd.Series(target).value_counts(normalize = True).cumsum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# splitter = StratifiedKFold(n_splits = 2, shuffle = True, random_state=666)\n",
    "# splits = list(splitter.split(uhs_train, target))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7.49 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "queries = []\n",
    "viewed_items = []\n",
    "viewed_domains = []\n",
    "num_queries = []\n",
    "for session in uhs_train:\n",
    "    session_viewed_items = dict()\n",
    "    session_viewed_domains = dict()\n",
    "    session_queries = []\n",
    "    s = 0\n",
    "    for event in session:\n",
    "        if event[\"event_type\"] == \"view\":\n",
    "            item_seen = event[\"event_info\"]\n",
    "            session_viewed_items[item_seen] = session_viewed_items.get(item_seen, 0) + 1\n",
    "            domain_seen = itemDomain[item_seen]\n",
    "            domain_seen = \"<UKNW>\" if domain_seen is None else domain_seen\n",
    "            session_viewed_domains[domain_seen] = session_viewed_domains.get(domain_seen, 0) + 1\n",
    "        else:\n",
    "            session_queries.append(event[\"event_info\"])\n",
    "            s += 1\n",
    "    viewed_items.append(session_viewed_items)\n",
    "    viewed_domains.append(session_viewed_domains)\n",
    "    queries.append(\" \".join(session_queries))\n",
    "    num_queries.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "sessions_length = np.vectorize(len)(uhs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_viewed = np.vectorize(len)(viewed_items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_queries = np.array(num_queries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "top1item = []\n",
    "isTopItem = np.zeros(len(target), int)\n",
    "containItemLabel = np.zeros(len(target), int)\n",
    "containDomainLabel = np.zeros(len(target), int)\n",
    "for i, (session, sessDoms, label, labelDom) in enumerate(zip(viewed_items, viewed_domains, target, domain)):\n",
    "    if len(session) > 0:\n",
    "        most = max(session, key = lambda k : session[k])\n",
    "    else:\n",
    "        most = -1\n",
    "        \n",
    "    labelDom\n",
    "    top1item.append(most)\n",
    "    isTopItem[i] = 1 if most == label else 0\n",
    "    containItemLabel[i] = 1 if label in session else 0\n",
    "    containDomainLabel[i] = 1 if labelDom in sessDoms else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.29388401187908886, 0.49340575027289474)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "containItemLabel.mean(), containDomainLabel.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.1568267245614927"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_score(target, top1item)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data (Bags of Words: Session items, Session domains, Session queries words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(curr):\n",
    "    # remove accent\n",
    "    curr = curr.str.normalize('NFKD').str.encode('ascii', errors='ignore').str.decode('utf-8')\n",
    "    # to lower case\n",
    "    curr = curr.str.lower()\n",
    "    # remove not alphanumerics or . ,\n",
    "    curr = curr.str.replace('[^a-zA-Z0-9.,]', ' ')\n",
    "    \n",
    "    # let , and . be the same char\n",
    "    curr = curr.str.replace('[.]', ',')\n",
    "    \n",
    "    # remove . , not between numbers\n",
    "    curr = curr.str.replace('(?<=[0-9])[,]+(?=[0-9])', '.')\n",
    "    curr = curr.str.replace('[,]', ' ')\n",
    "    \n",
    "    # set all digits to 0\n",
    "#     curr = curr.str.replace('[0-9]', '0')\n",
    "    \n",
    "    # remove some Pt plurals\n",
    "    curr = curr.str.replace('\\\\b([a-zA-Z]+[aeiouwy])(s)\\\\b', r'\\1')\n",
    "    \n",
    "    # remove 4 consec (same) letters to just one\n",
    "    curr = curr.str.replace(r'([a-zA-Z])\\1{3,}', r'\\1') # 3 is four? -> three of \\1 after first \\1... \n",
    "    \n",
    "    return curr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "sp1 = int(.8 * len(uhs_train))\n",
    "sp2 = len(uhs_train)\n",
    "uhs_all = np.concatenate([uhs_train, uhs_test], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information collected from user histories\n",
      "Items BOW created\n",
      "Domains BOW created\n",
      "Queries BOW created\n"
     ]
    }
   ],
   "source": [
    "# >> COLLECT LAST 20 ITEMS AND QUERIES\n",
    "l20_items = []\n",
    "l20_searches = []\n",
    "l20_domains = []\n",
    "num_queries = []\n",
    "num_items = []\n",
    "sess_len = []\n",
    "for session in uhs_all:\n",
    "    items = [event[\"event_info\"] for event in session if event[\"event_type\"] == \"view\"]\n",
    "    searches = [event[\"event_info\"] for event in session if event[\"event_type\"] == \"search\"]\n",
    "    l20_items.append({i : 1 for i in items[:-21:-1]})\n",
    "    l20_domains.append({itemDomain.get(i, \"<UNKN>\") : 1 for i in items[:-21:-1]})\n",
    "    l20_searches.append(\" \".join(searches))\n",
    "    num_queries.append(len(searches))\n",
    "    num_items.append(len(items))\n",
    "    sess_len.append(len(session))\n",
    "print(\"Information collected from user histories\")\n",
    "\n",
    "\n",
    "# >> ITEMS OH\n",
    "dv_items = DictVectorizer(dtype = int)\n",
    "items_bow = dv_items.fit_transform(l20_items)\n",
    "items_df = np.array(items_bow.sum(axis = 0))[0]\n",
    "# (items_df == 1).sum() \n",
    "items_bow = items_bow[:, items_df > 1] # removing can improve top1 but worst the overall\n",
    "# items_bow.shape\n",
    "print(\"Items BOW created\")\n",
    "\n",
    "# >> DOMAIN OH\n",
    "dv_domains = DictVectorizer()\n",
    "domains_bow = dv_domains.fit_transform(l20_domains)\n",
    "domains_df = np.array(domains_bow.sum(axis = 0))[0]\n",
    "# print((domains_df == 1).sum()) \n",
    "# I think I am missing a line where I should remove these domains that only appear once\n",
    "print(\"Domains BOW created\")\n",
    "\n",
    "# >> QUERIES OH\n",
    "normalized = normalize(pd.Series(l20_searches))\n",
    "cv_queries = CountVectorizer(binary = True, min_df = 5, max_df = .5)\n",
    "queries_bow = cv_queries.fit_transform(normalized)\n",
    "# docfreq = np.array(queries_bow.sum(axis = 0)).flatten() / queries_bow.shape[0]\n",
    "# inv_vocab = {v : k for k,v in cv_queries.vocabulary_.items()}\n",
    "# np.vectorize(inv_vocab.get)(np.argsort(docfreq)[-100:])\n",
    "# np.sort(docfreq)[-10:]\n",
    "print(\"Queries BOW created\")\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# queries_bowtf = TruncatedSVD(100).fit_transform(queries_bowtf)\n",
    "# items_bowtf = TruncatedSVD(100).fit_transform(items_bowtf)\n",
    "# domains_bowtf = TruncatedSVD(100).fit_transform(domains_bowtf)\n",
    "# X = np.concatenate([queries_bowtf, items_bowtf, domains_bowtf], axis = 1)\n",
    "# # works quite well\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "extras = np.column_stack([\n",
    "    num_queries,\n",
    "    num_items,\n",
    "    sess_len\n",
    "])\n",
    "extras = extras / extras.sum(axis = 0)\n",
    "\n",
    "# # these extras were wort -> top20 got dissipated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BOWs concatenated\n"
     ]
    }
   ],
   "source": [
    "# BOW NORMALIZATION\n",
    "# tft_queries = TfidfTransformer(norm='l2', use_idf=False, smooth_idf=True, sublinear_tf=False)\n",
    "# queries_bowtf = tft_queries.fit_transform(queries_bow)\n",
    "\n",
    "# tft_items = TfidfTransformer(norm='l2', use_idf=False, smooth_idf=True, sublinear_tf=False)\n",
    "# items_bowtf = tft_queries.fit_transform(items_bow)\n",
    "\n",
    "# tft_domains = TfidfTransformer(norm='l2', use_idf=False, smooth_idf=True, sublinear_tf=False)\n",
    "# domains_bowtf = tft_queries.fit_transform(domains_bow)\n",
    "# print(\"BOWs normalized\")\n",
    "\n",
    "# CONCATENATING\n",
    "X = sparse.hstack([queries_bow / 4, items_bow, domains_bow], format = 'csr')\n",
    "tft_X = TfidfTransformer(norm='l2', use_idf=True, smooth_idf=True, sublinear_tf=False)\n",
    "X = tft_X.fit_transform(X)\n",
    "print(\"BOWs concatenated\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Proportion of data used: 0.9286092897955355\n",
      "Data splitted\n"
     ]
    }
   ],
   "source": [
    "# >> SIMPLIFY PROBLEM (1): FILTER CLASSES\n",
    "# N_CLASSES = 1500\n",
    "N_CLASSES = 40000\n",
    "data_prop = cumsum.iloc[N_CLASSES]\n",
    "print(\"Proportion of data used:\", data_prop)\n",
    "sel_classes = cumsum.index[:N_CLASSES].values\n",
    "simple_target = target.copy()\n",
    "simple_target[~pd.Series(simple_target).isin(sel_classes)] = -1\n",
    "#pd.Series(simple_target).nunique()\n",
    "\n",
    "X_train, y_train, yalt_train = X[:sp1], target[:sp1], simple_target[:sp1]\n",
    "X_test, y_test, yalt_test = X[sp1:sp2], target[sp1:sp2], simple_target[sp1:sp2]\n",
    "X_full, y_full, yalt_full = X[:sp2], target, simple_target\n",
    "X_sub = X[sp2:]\n",
    "\n",
    "\n",
    "mask_train = containItemLabel[:sp1] == 1\n",
    "mask_full = containItemLabel == 1\n",
    "# mask_train = containDomainLabel[:sp1] == 1\n",
    "# mask_full = containDomainLabel == 1\n",
    "# top20 gets better sorted with containItem mask\n",
    "\n",
    "# SIMPLIFY PROBLEM (2): GETS EASIER SESSIONS \n",
    "X_train_simple = X_train[mask_train]\n",
    "y_train_simple = y_train[mask_train]\n",
    "yalt_train_simple = yalt_train[mask_train]\n",
    "\n",
    "X_full_simple = X_full[mask_full]\n",
    "y_full_simple = y_full[mask_full]\n",
    "yalt_full_simple = yalt_full[mask_full]\n",
    "\n",
    "print(\"Data splitted\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "NN = 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using KNN in a 5 fold split to generate candidates for all trainig data and avoid leakeage\n",
    "(yeah... I know there is more effient way to do this without using folds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 7 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "splitter = KFold(n_splits = 5, shuffle = False)\n",
    "splits = list(splitter.split(X_full, y_full, containItemLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12729781080197983\n",
      "0.12729781080197983\n",
      "0.12873791342441882\n",
      "0.12658534224029433\n",
      "0.12758979572078613\n",
      "Wall time: 12min 16s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "for j, (train_index, test_index) in enumerate(splits):\n",
    "    # test_index = test_index[:1000]\n",
    "    \n",
    "    X_train, y_train, yalt_train = X_full[train_index], y_full[train_index], yalt_full[train_index]\n",
    "    X_test, y_test, yalt_test = X_full[test_index], y_full[test_index], yalt_full[test_index]\n",
    "    \n",
    "    mask_train = containItemLabel[train_index] == 1\n",
    "    X_train_simple = X_train[mask_train]\n",
    "    y_train_simple = y_train[mask_train]\n",
    "    yalt_train_simple = yalt_train[mask_train]\n",
    "    \n",
    "    clf_knn_1 = KNeighborsClassifier(\n",
    "        n_neighbors=10,\n",
    "        n_jobs=-1,\n",
    "        leaf_size=30,\n",
    "        p=1,\n",
    "        metric='cosine',\n",
    "    ).fit(\n",
    "        X_train_simple,\n",
    "        yalt_train_simple,\n",
    "    )\n",
    "    \n",
    "    dist_test, ind_test = clf_knn_1.kneighbors(X_test, NN)\n",
    "    \n",
    "    recomms_test = np.zeros((len(y_test), NN), int)\n",
    "    for i in range(NN):\n",
    "        recomms_test[:, i] = yalt_train_simple[ind_test[:,i]]\n",
    "\n",
    "    knndists_test = dist_test[:, :NN]\n",
    "    \n",
    "    ind_df = pd.DataFrame(ind_test)\n",
    "    ind_df.to_csv(\"data/knn/inds_%d.csv\" % j, index = False, header = False)\n",
    "    \n",
    "    recomms_df = pd.DataFrame(recomms_test)\n",
    "    recomms_df.to_csv(\"data/knn/recomms_%d.csv\" % j, index = False, header = False)\n",
    "    \n",
    "    dists_df = pd.DataFrame(knndists_test)\n",
    "    dists_df.to_csv(\"data/knn/dists_%d.csv\" % j, index = False, header = False)\n",
    "    \n",
    "    # leave them splitted to create the good habit of generating the other features in the correct way\n",
    "    \n",
    "    print(accuracy_score(y_test, recomms_test[:, 0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generating candidates for the submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 6min 14s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# TRAIN KNN\n",
    "clf_knn_2 = KNeighborsClassifier(\n",
    "    n_neighbors=10,\n",
    "    n_jobs=-1,\n",
    "    #leaf_size=30,\n",
    "    p=1,\n",
    "    metric='cosine',\n",
    ").fit(\n",
    "    X_full_simple,\n",
    "    yalt_full_simple,\n",
    ")\n",
    "\n",
    "dist_sub, ind_sub = clf_knn_2.kneighbors(X_sub, NN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "recomms_sub = np.zeros((X_sub.shape[0], NN), int)\n",
    "for i in range(NN):\n",
    "    recomms_sub[:, i] = yalt_full_simple[ind_sub[:,i]]\n",
    "\n",
    "knndists_sub = dist_sub[:, :NN]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "ind_df = pd.DataFrame(ind_sub)\n",
    "ind_df.to_csv(\"data/knn/inds_sub.csv\", index = False, header = False)\n",
    "    \n",
    "recomms_df = pd.DataFrame(recomms_sub)\n",
    "recomms_df.to_csv(\"data/knn/recomms_sub.csv\", index = False, header = False)\n",
    "\n",
    "dists_df = pd.DataFrame(knndists_sub)\n",
    "dists_df.to_csv(\"data/knn/dists_sub.csv\", index = False, header = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
